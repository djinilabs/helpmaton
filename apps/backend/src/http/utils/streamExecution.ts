import { streamText } from "ai";

import {
  type StreamTextResultWithResolvedUsage,
  type TokenUsage,
} from "../../utils/conversationLogger";
import { uploadConversationFileFromBuffer } from "../../utils/s3";

import { extractTokenUsageAndCosts } from "./generationTokenExtraction";
import {
  isTimeoutError,
  createTimeoutError,
} from "./requestTimeout";
import {
  pipeAIStreamToResponse,
  type ProcessedFile,
} from "./streamAIPipeline";
import {
  handleStreamingError,
  handleResultExtractionError,
} from "./streamErrorHandling";
import {
  createEventTracking,
  type StreamEventTimestamps,
} from "./streamEventTracking";
import type { StreamRequestContext } from "./streamRequestContext";
import type { HttpResponseStream } from "./streamResponseStream";

/**
 * Result of executing a stream
 */
export interface StreamExecutionResult {
  streamResult: Awaited<ReturnType<typeof streamText>>;
  finalResponseText: string;
  tokenUsage: TokenUsage | undefined;
  generationTimeMs: number | undefined;
  generationStartedAt?: string; // ISO timestamp when generation started
  generationEndedAt?: string; // ISO timestamp when generation ended
  hasWrittenData: boolean; // Track if any data has been written to the stream
  eventTimestamps?: StreamEventTimestamps; // Event timestamps from model events
  generatedFiles?: ProcessedFile[]; // Files generated by the AI (images, etc.)
}

/**
 * Executes the AI stream and handles all processing
 * Returns the execution result or null if an error was handled
 */
export async function executeStream(
  context: StreamRequestContext,
  responseStream: HttpResponseStream,
  abortSignal?: AbortSignal
): Promise<StreamExecutionResult | null> {
  let fullStreamedText = "";
  let llmCallAttempted = false;
  let streamResult: Awaited<ReturnType<typeof streamText>> | undefined;
  let hasWrittenData = false; // Track if any data has been written to the stream

  // Create event tracking to capture timestamps from model events
  const eventTimestamps = createEventTracking();
  
  // Collect generated files
  const generatedFiles: ProcessedFile[] = [];

  // Create file processing callback
  const onFileChunk = async (file: {
    url?: string;
    base64?: string;
    uint8Array?: Uint8Array;
    mediaType?: string;
  }): Promise<ProcessedFile | null> => {
    try {
      // If we have an external URL, use it directly
      if (file.url) {
        console.log("[Stream Execution] Using external file URL:", file.url);
        return {
          url: file.url,
          mediaType: file.mediaType,
        };
      }

      // If we have embedded data, upload to S3
      let buffer: Buffer | undefined;
      if (file.base64) {
        console.log("[Stream Execution] Processing base64 file");
        buffer = Buffer.from(file.base64, "base64");
      } else if (file.uint8Array) {
        console.log("[Stream Execution] Processing uint8Array file");
        buffer = Buffer.from(file.uint8Array);
      }

      if (!buffer) {
        console.warn("[Stream Execution] File has no URL or embedded data, skipping");
        return null;
      }

      // Upload to S3
      const mediaType = file.mediaType || "application/octet-stream";
      console.log("[Stream Execution] Uploading file to S3:", {
        workspaceId: context.workspaceId,
        agentId: context.agentId,
        conversationId: context.conversationId,
        mediaType,
        size: buffer.length,
      });

      const s3Url = await uploadConversationFileFromBuffer(
        context.workspaceId,
        context.agentId,
        context.conversationId,
        buffer,
        mediaType
      );

      console.log("[Stream Execution] File uploaded to S3:", s3Url);
      return {
        url: s3Url,
        mediaType,
      };
    } catch (error) {
      console.error("[Stream Execution] Error processing file:", error);
      // Return null to skip this file rather than breaking the stream
      return null;
    }
  };

  let generationStartTime: number | undefined;
  let generationStartedAt: string | undefined;
  let generationTimeMs: number | undefined;
  let generationEndedAt: string | undefined;

  try {
    generationStartTime = Date.now();
    generationStartedAt = new Date().toISOString();
    streamResult = await pipeAIStreamToResponse(
      context.agent,
      context.model,
      context.modelMessages,
      context.tools,
      responseStream,
      (textDelta) => {
        fullStreamedText += textDelta;
      },
      abortSignal,
      () => {
        // Callback to track when data is actually written to the stream
        hasWrittenData = true;
      },
      eventTimestamps, // Pass event tracking to capture model event timestamps
      async (file) => {
        // Process file and collect it
        const processed = await onFileChunk(file);
        if (processed) {
          generatedFiles.push(processed);
        }
        return processed;
      }
    );
    
    // Use event timestamps if available, otherwise fall back to manual timing
    if (eventTimestamps.generationStartedAt) {
      generationStartedAt = eventTimestamps.generationStartedAt;
    }
    if (eventTimestamps.generationEndedAt) {
      generationEndedAt = eventTimestamps.generationEndedAt;
      if (generationStartTime !== undefined) {
        generationTimeMs = new Date(generationEndedAt).getTime() - generationStartTime;
      }
    } else if (generationStartTime !== undefined) {
      generationTimeMs = Date.now() - generationStartTime;
      generationEndedAt = new Date().toISOString();
    }
    llmCallAttempted = true;
  } catch (error) {
    // Check if this is a timeout error
    if (isTimeoutError(error)) {
      const timeoutError = createTimeoutError();
      
      // If data has already been written, we can't change the HTTP status code
      // Just write the error message to the stream and return null
      if (hasWrittenData) {
        await handleStreamingError(
          timeoutError,
          context,
          responseStream,
          llmCallAttempted
        );
        return null;
      }
      
      // If no data has been written, throw the error so the handler can set status 504
      throw timeoutError;
    }

    const handled = await handleStreamingError(
      error,
      context,
      responseStream,
      llmCallAttempted
    );
    if (handled === true) {
      return null;
    }
    throw error;
  }

  if (!streamResult) {
    throw new Error("LLM call succeeded but result is undefined");
  }

  // Extract text and usage
  let responseText: string;
  let usage: unknown;

  try {
    [responseText, usage] = await Promise.all([
      Promise.resolve(streamResult.text).then((t) => t || ""),
      Promise.resolve(streamResult.usage),
    ]);
  } catch (resultError) {
    const handled = await handleResultExtractionError(
      resultError,
      context,
      responseStream
    );
    if (handled) {
      return null;
    }
    throw resultError;
  }

  const finalResponseText = responseText || fullStreamedText;

  console.log("[Stream Execution] Final response text:", finalResponseText);

  // Extract token usage
  const totalUsage = await streamResult.totalUsage;
  const { tokenUsage } = extractTokenUsageAndCosts(
    { totalUsage } as unknown as StreamTextResultWithResolvedUsage,
    usage,
    context.finalModelName,
    context.endpointType as "test" | "stream"
  );

  return {
    streamResult,
    finalResponseText,
    tokenUsage,
    generationTimeMs,
    generationStartedAt,
    generationEndedAt,
    hasWrittenData,
    eventTimestamps,
    generatedFiles: generatedFiles.length > 0 ? generatedFiles : undefined,
  };
}

/**
 * Executes the stream for API Gateway (buffered) and returns the result
 * Errors are not handled here - they should be handled by the caller
 */
export async function executeStreamForApiGateway(
  context: StreamRequestContext,
  mockStream: HttpResponseStream,
  abortSignal?: AbortSignal
): Promise<StreamExecutionResult> {
  let fullStreamedText = "";
  const generationStartTime = Date.now();
  let generationStartedAt = new Date().toISOString();
  
  // Create event tracking for API Gateway path too
  const eventTimestamps = createEventTracking();
  
  // Collect generated files
  const generatedFiles: ProcessedFile[] = [];

  // Create file processing callback
  const onFileChunk = async (file: {
    url?: string;
    base64?: string;
    uint8Array?: Uint8Array;
    mediaType?: string;
  }): Promise<ProcessedFile | null> => {
    try {
      // If we have an external URL, use it directly
      if (file.url) {
        return {
          url: file.url,
          mediaType: file.mediaType,
        };
      }

      // If we have embedded data, upload to S3
      let buffer: Buffer | undefined;
      if (file.base64) {
        buffer = Buffer.from(file.base64, "base64");
      } else if (file.uint8Array) {
        buffer = Buffer.from(file.uint8Array);
      }

      if (!buffer) {
        return null;
      }

      // Upload to S3
      const mediaType = file.mediaType || "application/octet-stream";
      const s3Url = await uploadConversationFileFromBuffer(
        context.workspaceId,
        context.agentId,
        context.conversationId,
        buffer,
        mediaType
      );

      return {
        url: s3Url,
        mediaType,
      };
    } catch (error) {
      console.error("[Stream Execution] Error processing file:", error);
      return null;
    }
  };
  
  const streamResult = await pipeAIStreamToResponse(
    context.agent,
    context.model,
    context.modelMessages,
    context.tools,
    mockStream,
    (textDelta) => {
      fullStreamedText += textDelta;
    },
    abortSignal,
    undefined, // No need to track data written for API Gateway (buffered)
    eventTimestamps, // Pass event tracking
    async (file) => {
      // Process file and collect it
      const processed = await onFileChunk(file);
      if (processed) {
        generatedFiles.push(processed);
      }
      return processed;
    }
  );

  if (!streamResult) {
    throw new Error("LLM call succeeded but result is undefined");
  }

  // Use event timestamps if available
  if (eventTimestamps.generationStartedAt) {
    generationStartedAt = eventTimestamps.generationStartedAt;
  }
  let generationEndedAt = new Date().toISOString();
  if (eventTimestamps.generationEndedAt) {
    generationEndedAt = eventTimestamps.generationEndedAt;
  }
  const generationTimeMs = new Date(generationEndedAt).getTime() - generationStartTime;

  // Extract text and usage
  const [responseText, usage] = await Promise.all([
    streamResult.text,
    streamResult.usage,
  ]);

  const finalResponseText = responseText || fullStreamedText;

  // Extract token usage
  const totalUsage = await streamResult.totalUsage;
  const { tokenUsage } = extractTokenUsageAndCosts(
    { totalUsage } as unknown as StreamTextResultWithResolvedUsage,
    usage,
    context.finalModelName,
    context.endpointType as "test" | "stream"
  );

  return {
    streamResult,
    finalResponseText,
    tokenUsage,
    generationTimeMs,
    generationStartedAt,
    generationEndedAt,
    hasWrittenData: true, // For API Gateway, we assume data was written if we got here
    eventTimestamps,
    generatedFiles: generatedFiles.length > 0 ? generatedFiles : undefined,
  };
}
